{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Assignment 2\n",
    "* Students:\n",
    "    * Davíð Freyr Björnsson\n",
    "    * Eric Guldbrand\n",
    "* Time spent per person: 12 hours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** _A company is considering using a system that will allow management to track staff behaviour in real-time, gather data on when and who they email, who accesses and edits files, and who meets whom when. The HR (human resources, or personnel) department at the company is in favour of introducing this new system and believes it will benefit the staff at the company. The company's management is also in favour. Discuss whether introducing this system raises potential ethical issues._\n",
    "\n",
    "One useful approach to this issue is to look\n",
    "at the benefits and costs that the stakeholders of the company would incur.\n",
    "This system would benefit HR and the company's management in several ways.\n",
    "Good communication within a company is vital for a company's success.\n",
    "The HR department could see if there are individuals or groups within\n",
    "the company that should be communicating but are not. They could then\n",
    "potentially rectify the situation before the lack of communication has adverse\n",
    "consequences.\n",
    "\n",
    "This data collection would also beneficial for getting some\n",
    "indication of who's productive and who's not. Workers who are productive\n",
    "tend to access more files, email and meet more people. But this is an imperfect\n",
    "indicator of performance so more in-depth analysis of an individuals performance\n",
    "would have to be performed as well.\n",
    "\n",
    "But from the standpoint of the individual worker, the introduction of this\n",
    "system may not seem like a step in the right direction. The feeling of someone\n",
    "constantly looking over your shoulder can give the impression that the company\n",
    "doesn't trust you to do a good enough job on your own. Put in another way; if the comapny's interest don't align well enough with yours, you will probably feel like you are not on the same team then as the management and HR. That would have a bad effect on the company's morale and performance.\n",
    "\n",
    "It then seems that HR and management have only something to gain and others\n",
    "within the company have only something to lose. But this all depends on how\n",
    "this system will be implemented and used. The company's management and HR could\n",
    "have a policy of trusting the workers to do a good job (i.e. not looking and\n",
    "analyzing the data on a regular basis) but using it only in cases of dispute,\n",
    "where it's not clear who's responsible. This would benefit HR and management\n",
    "but also employees, as more data is beneficial in the case of disputes.\n",
    "\n",
    "The final outcome of the introduction of this system comes down to whether or not we can trust the HR department and the company's management\n",
    "not to misuse the data. As such, there needs to be clear definitions of how the data will be used or employees could come to see unreasonable many negative effects in the future.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# All properties with their room count and living area listed are included.\n",
    "\n",
    "from lxml import html\n",
    "from lxml import etree\n",
    "import requests\n",
    "from io import StringIO, BytesIO\n",
    "import re\n",
    "\n",
    "houseSizes = []\n",
    "housePrices = []\n",
    "houseRooms = []\n",
    "\n",
    "parser = etree.HTMLParser(encoding=\"UTF-8\")\n",
    "\n",
    "# Iterate over the pagination. How many pages to go through is set manually.\n",
    "for pageIndex in range(1, 11):\n",
    "    # Request the current page and parse it\n",
    "    url = 'https://www.hemnet.se/salda/bostader?location_ids%5B%5D=474180&page=' + str(pageIndex) + '&sold_age=all'\n",
    "    data = requests.get(url).text\n",
    "\n",
    "    tree = etree.parse(StringIO(data), parser)\n",
    "\n",
    "    # List of all property divs on the current page\n",
    "    propertyListings = tree.xpath('//div[@class=\"sold-property-listing\"]')\n",
    "\n",
    "    for property in propertyListings:\n",
    "        # Select element that has sizes and rooms\n",
    "        sizesRooms = property.xpath('./div[@class=\"sold-property-listing__size\"]/div[@class=\"clear-children\"]/div[@class=\"sold-property-listing__subheading sold-property-listing--left\"]/text()')\n",
    "        # Some properties doesn't have this exact html layout\n",
    "        # because they have no houses built on them. Skip those.\n",
    "        if (len(sizesRooms) == 0):\n",
    "            continue\n",
    "        sizesRooms = list(map(lambda x: re.sub(\"\\xa0\", \"\", x), sizesRooms))\n",
    "        # Get the living area and convert to integer\n",
    "        size = list(map(lambda x: re.findall(\"([0-9]*,*[0-9]*)m²\", x), sizesRooms))[0][0]\n",
    "        size = round(float(size.replace(\",\", \".\")))\n",
    "\n",
    "        def calcRoomCount(element):\n",
    "            finds = re.findall(\"([0-9]*)rum\", element)\n",
    "            if (len(finds) == 0):\n",
    "                 return \"0\"\n",
    "            return finds[0]\n",
    "\n",
    "        # Get room count\n",
    "        rooms = list(map(calcRoomCount, sizesRooms))[0]\n",
    "        rooms = int(round(float(rooms)))\n",
    "        # If there were 0 rooms, lets ignore this data point\n",
    "        if (rooms == 0):\n",
    "            continue\n",
    "\n",
    "        # Get the price of this property\n",
    "        price = property.xpath('./div[@class=\"sold-property-listing__price\"]/div[@class=\"clear-children\"]/span[@class=\"sold-property-listing__subheading sold-property-listing--left\"]/text()')\n",
    "        price = price[0]\n",
    "        price = re.sub(\"\\xa0\", \"\", price)\n",
    "        price = re.sub(\"\\n(.*)Slutpris \", \"\", price)\n",
    "        price = re.sub(\" kr\\n\", \"\", price)\n",
    "\n",
    "        housePrices.append(int(price))\n",
    "        houseSizes.append(int(size))\n",
    "        houseRooms.append(int(rooms))\n",
    "\n",
    "if (len(housePrices) != len(houseSizes) or len(housePrices) != len(houseRooms)):\n",
    "    print(\"Assert array lengths failed.\", len(housePrices), len(houseSizes), len(houseRooms))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to collect data for all properties listed as sold in Landvetter at anytime, as listed by hemnet.se. Properties without anything built on them and properties without floor area information were excluded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.** _What are the values of the slope and intercept of the regression line?_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create a pandas dataframe\n",
    "data = {'Area': houseSizes, 'Price': housePrices}\n",
    "df = pd.DataFrame(data)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "df['Price'] = df['Price']/1000000\n",
    "X = df['Area'].values.reshape(-1, 1)\n",
    "y = df['Price'].values.reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Perform linear regression using Scikit-learn\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X, y)\n",
    "\n",
    "print(\"The value of the slope is\", round(regressor.intercept_[0],2))\n",
    "print(\"The value of the coefficient is\", round(regressor.coef_[0][0],2))\n",
    "\n",
    "# Let's predict the prices based on the living area\n",
    "y_pred = regressor.predict(X)\n",
    "plt.scatter(X, y, color = 'gray');\n",
    "plt.plot(X, y_pred, color = 'red', linewidth = 2);\n",
    "plt.title('Living area vs price')\n",
    "plt.xlabel('Living area ($m^2$)')\n",
    "plt.ylabel('Price (Million SEK)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** _Use this model to predict the selling prices of houses which have living area\n",
    "100 $m^2$, 150 $m^2$ and 200 $m^2$._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "print(\"The predicted selling price (in millions of SEK) of a house with a living area 100 m^2 is\",\n",
    "round(regressor.predict([[100]])[0][0]), 2)\n",
    "\n",
    "print(\"The predicted selling price (in millions of SEK) of a house with a living area 150 m^2 is\",\n",
    "round(regressor.predict([[150]])[0][0]), 2)\n",
    "\n",
    "print(\"The predicted selling price (in millions of SEK) of a house with a living area 200 m^2 is\",\n",
    "round(regressor.predict([[200]])[0][0]), 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** _Draw a residual plot._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "y_residual = y - y_pred\n",
    "plt.scatter(X, y_residual)\n",
    "plt.title('Living area vs residual')\n",
    "plt.xlabel('Living area ($m^2$)')\n",
    "plt.ylabel('Residual (Million SEK)')\n",
    "\n",
    "frame = pd.DataFrame({'X': X.squeeze(), 'res': y_residual.squeeze()})\n",
    "frame.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d.** _Discuss the results, and how the model could be improved._\n",
    "\n",
    "Our linear regression model tends to underestimate the prices of larger houses. This can be clearly seen in the residual plot. A possible reason for this is that larger houses may tend to be built where the price of land is low, but the parameters of our linear regression model are more influenced by smaller houses (less than $140 m^2$) than larger houses (greater than $140 m^2$). As such the \"location value\" is trained for smaller houses in more central locations.\n",
    "\n",
    "An improvement for the model could then be to also consider some measurement of location or land value, or including more large houses in the training set. It could also be considered to only look at a specific type of house, such as villas, in comparison to other types of housing.\n",
    "\n",
    " ## Normal distribution of residuals\n",
    "An important assumption of the linear regression model is that the residuals are normally distributed. Let's plot a histogram:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "plt.hist(y_residual);\n",
    "plt.xlabel(\"Residuals\");\n",
    "plt.ylabel(\"Frequency\");\n",
    "plt.title(\"Residual histogram\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals seem to be normally distributed. Let's confirm that by conducting the Shapiro-Wilk, D’Agostino’s K^2 and Anderson-Darling test. The null hypothesis in all three test is that the data comes from a normal distribution.\n",
    "The Shapiro-Wilk test is best suited for datasets with a thousand observations or less (smaller datasets).\n",
    "The $D’Agostino’s K^2$ test looks toward the kurtosis and skewness to determine if the data distribution departs from the normal distribution. An interesting feature of the Anderson-Darling test is that it returns a list of p-values that can be compared to the threshold value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "statShap, pShap = shapiro(y_residual)\n",
    "print(\"The p-value for the Shapiro-Wilk test is: \", round(pShap, 4));\n",
    "\n",
    "from scipy.stats import normaltest\n",
    "statDag, pDag = normaltest(y_residual)\n",
    "print(\"The p-value for the D’Agostino’s K^2 test is: \", round(pDag[0], 4));\n",
    "\n",
    "from scipy.stats import anderson\n",
    "pAnd = anderson(y_residual)\n",
    "print(\"The results of the Anderson Darling test are: \")\n",
    "\n",
    "for i in range(len(pAnd)):\n",
    "    print(\"For the significance level of\", pAnd.significance_level[i], \"%\", \"the corresponding p-value is\", pAnd.critical_values[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the Shapiro-Wilk test and the D'Agostino's K^2 test reject the null hypothesis at a significance level of 5% that the data comes from a normal distribution. But the Anderson-Darling test fails to reject the null hypothesis at a significance level of 15%, 10% and 5%. This suggests that the distribution of the residuals has some features of a normal distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** _Use a confusion matrix and 5-fold cross-validation to evaluate the use logistic regression to classify the iris data set._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "class_names = iris.target_names\n",
    "\n",
    "# Check if there are equal number of instances in each class\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are equal number of instances in each class, no normalization of the data is needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "def kfoldEval(classifier, k = 5):\n",
    "    kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=1)\n",
    "    y_pred_total = []\n",
    "    y_test_total = []\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        classifier = classifier.fit(X_train, y_train)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        #plot_confusion_matrix(y_test, y_pred, classes=class_names,title='Non-normalized confusion matrix');\n",
    "        y_pred_total.extend(y_pred)\n",
    "        y_test_total.extend(y_test)\n",
    "    return {\"predictions\": y_pred_total, \"tests\": y_test_total}\n",
    "\n",
    "evaluation = kfoldEval(LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000, multi_class='multinomial'), 5)\n",
    "plot_confusion_matrix(evaluation['predictions'], evaluation['tests'], classes=class_names,title='Non-normalized confusion matrix');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.__ _Comparing classification models_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "listOfClassifiers = [\n",
    "(LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000, multi_class='multinomial'), 'Logistic'),\n",
    "(KNeighborsClassifier(weights='uniform'), 'KNeighbors Uniform'),  (KNeighborsClassifier(weights='distance'), 'KNeighbors Distance'),\n",
    "(SVC(gamma='auto'), 'SVC')\n",
    "]\n",
    "\n",
    "for cdata in listOfClassifiers:\n",
    "    classifier = cdata[0]\n",
    "    eval = kfoldEval(classifier, 5)\n",
    "    plot_confusion_matrix(eval['predictions'], eval['tests'], classes=class_names, title=cdata[1])\n",
    "\n",
    "    cm = np.array(confusion_matrix(eval['predictions'], eval['tests']))\n",
    "    true_pos = np.sum(np.diag(cm))\n",
    "    false_pos = np.sum(np.sum(cm, axis=0)) - true_pos\n",
    "    false_neg = np.sum(np.sum(cm, axis=1)) - true_pos\n",
    "\n",
    "    # Different types of measurement.\n",
    "    # Turns out that for multiclass confusion matrices, they are all the same.\n",
    "    multi_class_accuracy = true_pos / np.sum(cm)\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "    recall = true_pos / (true_pos + false_neg)\n",
    "    f1 = 2 * true_pos / (2 * true_pos + false_pos + false_neg)\n",
    "    print(round(precision, 3), round(recall, 3), round(f1, 3), round(multi_class_accuracy, 3))\n",
    "    #print(true_pos, false_pos, false_neg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrices show that the different models perform quite similiarily, and all quite well, on the iris dataset. The KNeighbors classifiers just make one misclassification more than the other classifiers.\n",
    "\n",
    "All misclassifications are between versicolor and virginica. Setosa is always correctly classified, and no other species is wrongly classified as setosa.\n",
    "\n",
    "In terms of accuracy the logistic regression and SVC classifiers perform the best and have an accuracy of $96.7%$, while the KNeighbors classifiers perform slightly worse with accuracy of $96%$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Bibliography\n",
    "(2019, August 8). A Gentle Introduction to Normality Tests in Python. Retrieved from https://machinelearningmastery.com/a-gentle-introduction-to-normality-tests-in-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
